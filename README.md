# ğŸš€ Customer 360 Data Platform

A **production-grade, cloud-native data engineering platform** built on AWS that ingests real-time and batch data, applies layered transformations, models it as a star schema, and serves analytics dashboards â€” with full orchestration, data quality, monitoring, and Infrastructure as Code.

### ğŸ”— <a href="https://customer-360-platform-hnsr2z6bpmrqs43d8qnarf.streamlit.app/" target="_blank">Live Demo â†’</a>



## ğŸ“ Architecture Overview

```mermaid
flowchart LR
    subgraph Ingestion
        A[Kinesis Producer] --> B[Kinesis Data Stream]
        C[Batch CSV Uploader] --> D[S3 Raw Layer]
        B --> E[Lambda Consumer] --> D
    end

    subgraph DataLake["Data Lake (S3)"]
        D --> F[Raw Layer<br/>JSON/CSV]
        F --> G[Clean Layer<br/>Parquet]
        G --> H[Curated Layer<br/>Star Schema Parquet]
    end

    subgraph ETL["ETL (AWS Glue)"]
        F -->|raw_to_clean.py| G
        G -->|clean_to_curated.py| H
        H -->|curated_to_redshift.py| I
    end

    subgraph Warehouse
        I[Amazon Redshift] --> J[BI Dashboards]
    end

    subgraph Orchestration
        K[EventBridge Schedule] --> L[Step Functions]
        L --> M{Quality Gate}
        M -->|PASS| H
        M -->|FAIL| N[Quarantine + Alert]
    end

    subgraph Monitoring
        O[CloudWatch Alarms] --> P[SNS Alerts]
        Q[CloudWatch Dashboard]
    end
```

---

## ğŸ—‚ï¸ Project Structure

```
customer-360-platform/
â”œâ”€â”€ config/                          # Centralized pipeline configuration
â”‚   â””â”€â”€ pipeline_config.yaml
â”œâ”€â”€ ingestion/                       # Data ingestion layer
â”‚   â”œâ”€â”€ kinesis/                     # Real-time streaming
â”‚   â”‚   â”œâ”€â”€ producer.py              # Clickstream event producer
â”‚   â”‚   â””â”€â”€ consumer_lambda.py       # Lambda â†’ S3 writer
â”‚   â””â”€â”€ batch/                       # Batch uploads
â”‚       â””â”€â”€ upload_csv.py            # CSV uploader with multipart
â”œâ”€â”€ data_lake/                       # Data lake management
â”‚   â”œâ”€â”€ s3_structure.md              # 3-layer architecture docs
â”‚   â””â”€â”€ glue_catalog.py              # Glue Data Catalog setup
â”œâ”€â”€ etl/                             # AWS Glue ETL jobs
â”‚   â””â”€â”€ glue_jobs/
â”‚       â”œâ”€â”€ raw_to_clean.py          # Schema enforcement, dedup, normalization
â”‚       â”œâ”€â”€ clean_to_curated.py      # Star schema dimensions/facts, business logic
â”‚       â””â”€â”€ curated_to_redshift.py   # Load to warehouse via COPY + MERGE
â”œâ”€â”€ warehouse/                       # Amazon Redshift
â”‚   â”œâ”€â”€ ddl/
â”‚   â”‚   â”œâ”€â”€ create_schema.sql        # Star schema DDL (facts + dimensions)
â”‚   â”‚   â””â”€â”€ staging_tables.sql       # Staging tables + upsert procedures
â”‚   â”œâ”€â”€ queries/
â”‚   â”‚   â””â”€â”€ analytics_queries.sql    # 10 production analytics queries
â”‚   â””â”€â”€ maintenance/
â”‚       â””â”€â”€ vacuum_analyze.sql       # VACUUM, ANALYZE, health monitoring
â”œâ”€â”€ orchestration/                   # Pipeline orchestration
â”‚   â”œâ”€â”€ step_functions/
â”‚   â”‚   â”œâ”€â”€ pipeline_state_machine.json  # Full pipeline ASL definition
â”‚   â”‚   â””â”€â”€ deploy_state_machine.py      # Deployer script
â”‚   â””â”€â”€ eventbridge/
â”‚       â””â”€â”€ schedule_rule.py         # Hourly/daily schedule triggers
â”œâ”€â”€ data_quality/                    # Data quality framework
â”‚   â”œâ”€â”€ schemas/                     # JSON schemas for validation
â”‚   â”‚   â”œâ”€â”€ clickstream_schema.json
â”‚   â”‚   â”œâ”€â”€ customer_schema.json
â”‚   â”‚   â”œâ”€â”€ product_schema.json
â”‚   â”‚   â””â”€â”€ transaction_schema.json
â”‚   â””â”€â”€ validators/
â”‚       â”œâ”€â”€ schema_validator.py      # JSON schema validation engine
â”‚       â”œâ”€â”€ data_checks.py           # Null, dup, range, freshness, referential
â”‚       â””â”€â”€ quality_reporter.py      # Report generation + S3 quarantine
â”œâ”€â”€ monitoring/                      # Monitoring & alerting
â”‚   â”œâ”€â”€ cloudwatch/
â”‚   â”‚   â”œâ”€â”€ alarms.py                # 7 CloudWatch alarms
â”‚   â”‚   â””â”€â”€ dashboards.py           # Operational dashboard
â”‚   â””â”€â”€ sns/
â”‚       â””â”€â”€ notifications.py        # SNS topic + email subscription
â”œâ”€â”€ security/                       # Security & governance
â”‚   â”œâ”€â”€ iam/
â”‚   â”‚   â”œâ”€â”€ policies.json           # Least-privilege IAM policies
â”‚   â”‚   â””â”€â”€ roles.py                # IAM role creation
â”‚   â”œâ”€â”€ encryption/
â”‚   â”‚   â””â”€â”€ kms_setup.py            # KMS key with rotation
â”‚   â””â”€â”€ s3/
â”‚       â””â”€â”€ bucket_policies.json    # S3 bucket policies (enforce encryption)
â”œâ”€â”€ sample_data/                    # Data generators
â”‚   â””â”€â”€ generators/
â”‚       â”œâ”€â”€ generate_customers.py    # 5K customer records
â”‚       â”œâ”€â”€ generate_products.py     # 200 product catalog
â”‚       â”œâ”€â”€ generate_transactions.py # 50K transactions
â”‚       â””â”€â”€ generate_clickstream.py  # 100K clickstream events
â”œâ”€â”€ terraform/                      # Infrastructure as Code
â”‚   â”œâ”€â”€ main.tf                     # Root module (all wired together)
â”‚   â”œâ”€â”€ variables.tf                # Input variables
â”‚   â”œâ”€â”€ outputs.tf                  # Output values
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ s3/main.tf              # S3 buckets with lifecycle
â”‚       â”œâ”€â”€ kms/main.tf             # KMS encryption key
â”‚       â”œâ”€â”€ kinesis/main.tf         # Kinesis Data Stream
â”‚       â”œâ”€â”€ lambda/main.tf          # Lambda + Kinesis trigger + DLQ
â”‚       â”œâ”€â”€ glue/main.tf            # Glue jobs + catalog + security config
â”‚       â”œâ”€â”€ redshift/main.tf        # Redshift cluster (VPC, encrypted)
â”‚       â”œâ”€â”€ step_functions/main.tf  # Step Functions + EventBridge
â”‚       â””â”€â”€ monitoring/main.tf      # CloudWatch + SNS
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_data_quality.py        # Unit tests for quality checks
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

---

## âš™ï¸ Tech Stack

| Layer | AWS Service | Purpose |
|---|---|---|
| **Streaming Ingestion** | Kinesis Data Streams | Real-time clickstream events |
| **Batch Ingestion** | S3 + Python uploader | CSV & bulk file uploads |
| **Data Lake** | S3 (3-layer) | Raw â†’ Clean â†’ Curated |
| **Data Catalog** | AWS Glue Data Catalog | Schema registry & partitions |
| **ETL** | AWS Glue (PySpark) | Distributed data transformations |
| **Warehouse** | Amazon Redshift | Star schema analytics |
| **Orchestration** | Step Functions + EventBridge | End-to-end pipeline workflow |
| **Data Quality** | Custom Python + JSON Schema | Validation, quarantine, reporting |
| **Monitoring** | CloudWatch + SNS | Alarms, dashboards, email alerts |
| **Security** | IAM, KMS, S3 policies | Least privilege, encryption at rest |
| **IaC** | Terraform | Full infrastructure provisioning |

---

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Generate Sample Data
```bash
cd sample_data/generators
python generate_customers.py --count 5000
python generate_products.py --count 200
python generate_transactions.py --count 50000
python generate_clickstream.py --count 100000
```

### 3. Deploy Infrastructure (Terraform)
```bash
cd terraform
terraform init
terraform plan -var-file="dev.tfvars"
terraform apply -var-file="dev.tfvars"
```

### 4. Run Data Quality Tests
```bash
pytest tests/ -v
```

---

## ğŸ”’ Security Features

- **SSE-KMS** encryption for all S3 buckets and Redshift
- **Least-privilege IAM roles** per service (Lambda, Glue, Redshift, Step Functions)
- **S3 bucket policies** deny unencrypted uploads and insecure transport
- **VPC isolation** for Redshift cluster
- **Automatic key rotation** for KMS CMK
- **Public access blocked** on all S3 buckets

---

## ğŸ“Š Analytics Queries

The warehouse includes **10 production-ready analytics queries**:
1. Customer Lifetime Value (CLV) with segmentation
2. Monthly revenue trends with MoM growth
3. Cohort retention analysis
4. Product performance matrix
5. Customer churn risk scoring
6. Channel conversion funnel
7. RFM segmentation
8. Product affinity analysis (market basket)
9. Geographic revenue heatmap
10. Time-series demand forecasting base

---

## ğŸ“ˆ Monitoring

The platform includes a CloudWatch dashboard tracking:
- Pipeline execution success/failure rates
- Glue job durations
- Kinesis stream health (incoming records, iterator age)
- Lambda consumer invocations and errors
- Redshift CPU and disk utilization
- S3 data lake growth

---

## ğŸ“„ License

This project is for educational and portfolio purposes.
